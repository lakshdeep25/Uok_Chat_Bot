{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wHgQTu4naeMD"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BLIS support requires blis: pip install blis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangdetect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load spaCy model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\laksh\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\laksh\\anaconda3\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32mc:\\Users\\laksh\\anaconda3\\Lib\\site-packages\\spacy\\compat.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[0;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[1;32mc:\\Users\\laksh\\anaconda3\\Lib\\site-packages\\thinc\\api.py:23\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     configure_normal_init,\n\u001b[0;32m     18\u001b[0m     glorot_uniform_init,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     zero_init,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     LSTM,\n\u001b[0;32m     25\u001b[0m     CauchySimilarity,\n\u001b[0;32m     26\u001b[0m     ClippedLinear,\n\u001b[0;32m     27\u001b[0m     Dish,\n\u001b[0;32m     28\u001b[0m     Dropout,\n\u001b[0;32m     29\u001b[0m     Embed,\n\u001b[0;32m     30\u001b[0m     Gelu,\n\u001b[0;32m     31\u001b[0m     HardSigmoid,\n\u001b[0;32m     32\u001b[0m     HardSwish,\n\u001b[0;32m     33\u001b[0m     HardSwishMobilenet,\n\u001b[0;32m     34\u001b[0m     HardTanh,\n\u001b[0;32m     35\u001b[0m     HashEmbed,\n\u001b[0;32m     36\u001b[0m     LayerNorm,\n\u001b[0;32m     37\u001b[0m     Linear,\n\u001b[0;32m     38\u001b[0m     Logistic,\n\u001b[0;32m     39\u001b[0m     Maxout,\n\u001b[0;32m     40\u001b[0m     Mish,\n\u001b[0;32m     41\u001b[0m     MultiSoftmax,\n\u001b[0;32m     42\u001b[0m     MXNetWrapper,\n\u001b[0;32m     43\u001b[0m     ParametricAttention,\n\u001b[0;32m     44\u001b[0m     ParametricAttention_v2,\n\u001b[0;32m     45\u001b[0m     PyTorchLSTM,\n\u001b[0;32m     46\u001b[0m     PyTorchRNNWrapper,\n\u001b[0;32m     47\u001b[0m     PyTorchWrapper,\n\u001b[0;32m     48\u001b[0m     PyTorchWrapper_v2,\n\u001b[0;32m     49\u001b[0m     PyTorchWrapper_v3,\n\u001b[0;32m     50\u001b[0m     Relu,\n\u001b[0;32m     51\u001b[0m     ReluK,\n\u001b[0;32m     52\u001b[0m     Sigmoid,\n\u001b[0;32m     53\u001b[0m     Softmax,\n\u001b[0;32m     54\u001b[0m     Softmax_v2,\n\u001b[0;32m     55\u001b[0m     SparseLinear,\n\u001b[0;32m     56\u001b[0m     SparseLinear_v2,\n\u001b[0;32m     57\u001b[0m     Swish,\n\u001b[0;32m     58\u001b[0m     TensorFlowWrapper,\n\u001b[0;32m     59\u001b[0m     TorchScriptWrapper_v1,\n\u001b[0;32m     60\u001b[0m     add,\n\u001b[0;32m     61\u001b[0m     array_getitem,\n\u001b[0;32m     62\u001b[0m     bidirectional,\n\u001b[0;32m     63\u001b[0m     chain,\n\u001b[0;32m     64\u001b[0m     clone,\n\u001b[0;32m     65\u001b[0m     concatenate,\n\u001b[0;32m     66\u001b[0m     expand_window,\n\u001b[0;32m     67\u001b[0m     keras_subclass,\n\u001b[0;32m     68\u001b[0m     list2array,\n\u001b[0;32m     69\u001b[0m     list2padded,\n\u001b[0;32m     70\u001b[0m     list2ragged,\n\u001b[0;32m     71\u001b[0m     map_list,\n\u001b[0;32m     72\u001b[0m     noop,\n\u001b[0;32m     73\u001b[0m     padded2list,\n\u001b[0;32m     74\u001b[0m     premap_ids,\n\u001b[0;32m     75\u001b[0m     pytorch_to_torchscript_wrapper,\n\u001b[0;32m     76\u001b[0m     ragged2list,\n\u001b[0;32m     77\u001b[0m     reduce_first,\n\u001b[0;32m     78\u001b[0m     reduce_last,\n\u001b[0;32m     79\u001b[0m     reduce_max,\n\u001b[0;32m     80\u001b[0m     reduce_mean,\n\u001b[0;32m     81\u001b[0m     reduce_sum,\n\u001b[0;32m     82\u001b[0m     remap_ids,\n\u001b[0;32m     83\u001b[0m     remap_ids_v2,\n\u001b[0;32m     84\u001b[0m     residual,\n\u001b[0;32m     85\u001b[0m     resizable,\n\u001b[0;32m     86\u001b[0m     siamese,\n\u001b[0;32m     87\u001b[0m     sigmoid_activation,\n\u001b[0;32m     88\u001b[0m     softmax_activation,\n\u001b[0;32m     89\u001b[0m     strings2arrays,\n\u001b[0;32m     90\u001b[0m     tuplify,\n\u001b[0;32m     91\u001b[0m     uniqued,\n\u001b[0;32m     92\u001b[0m     with_array,\n\u001b[0;32m     93\u001b[0m     with_array2d,\n\u001b[0;32m     94\u001b[0m     with_cpu,\n\u001b[0;32m     95\u001b[0m     with_debug,\n\u001b[0;32m     96\u001b[0m     with_flatten,\n\u001b[0;32m     97\u001b[0m     with_flatten_v2,\n\u001b[0;32m     98\u001b[0m     with_getitem,\n\u001b[0;32m     99\u001b[0m     with_list,\n\u001b[0;32m    100\u001b[0m     with_nvtx_range,\n\u001b[0;32m    101\u001b[0m     with_padded,\n\u001b[0;32m    102\u001b[0m     with_ragged,\n\u001b[0;32m    103\u001b[0m     with_reshape,\n\u001b[0;32m    104\u001b[0m     with_signpost_interval,\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    107\u001b[0m     CategoricalCrossentropy,\n\u001b[0;32m    108\u001b[0m     CosineDistance,\n\u001b[0;32m    109\u001b[0m     L2Distance,\n\u001b[0;32m    110\u001b[0m     SequenceCategoricalCrossentropy,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    113\u001b[0m     Model,\n\u001b[0;32m    114\u001b[0m     change_attr_values,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     wrap_model_recursive,\n\u001b[0;32m    119\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\laksh\\anaconda3\\Lib\\site-packages\\thinc\\layers\\__init__.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclipped_linear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClippedLinear, HardSigmoid, HardTanh, ReluK\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcatenate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concatenate\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdish\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dish\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdropout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dropout\n",
      "File \u001b[1;32mc:\\Users\\laksh\\anaconda3\\Lib\\site-packages\\thinc\\layers\\concatenate.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_width\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnoop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m noop\n\u001b[1;32m---> 21\u001b[0m NUMPY_OPS \u001b[38;5;241m=\u001b[39m NumpyOps()\n\u001b[0;32m     24\u001b[0m InT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mAny)\n\u001b[0;32m     25\u001b[0m OutT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mUnion[Array2d, Sequence[Array2d], Ragged])\n",
      "File \u001b[1;32mc:\\Users\\laksh\\anaconda3\\Lib\\site-packages\\thinc\\backends\\numpy_ops.pyx:63\u001b[0m, in \u001b[0;36mthinc.backends.numpy_ops.NumpyOps.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: BLIS support requires blis: pip install blis"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6YZnoGDj3-W"
   },
   "source": [
    "# Step 1: Data Extraction\n",
    "This step involves extracting raw data from various file formats, such as PDFs and CSV files, and saving the data in a structured format (JSON).\n",
    "\n",
    "Libraries and Technologies Used:\n",
    "pdfminer.six (for extracting text from PDF files):\n",
    "\n",
    "**Purpose**: pdfminer.six is a Python library used to extract text from PDF files. It parses the PDF content and converts it into a string format.\n",
    "\n",
    "**Usage**: Extracts text from the Affiliated-Colleges.pdf and UOK-Admission Notification 2022-23.pdf.\n",
    "\n",
    "**Documentation**: pdfminer.six\n",
    "pandas (for extracting and reading CSV files):\n",
    "\n",
    "**Purpose**: pandas is a powerful library for data manipulation and analysis. It can handle structured data and is used here to read the uok_data.csv.txt file.\n",
    "\n",
    "**Usage**: Reads CSV files and extracts the contents as a string.\n",
    "Documentation: pandas\n",
    "json (for saving extracted data into a structured format):\n",
    "\n",
    "**Purpose**: The built-in json library is used to save the extracted and structured data into a JSON file for further processing.\n",
    "\n",
    "**Usage**: Serializes Python objects (such as the extracted data dictionary) into a JSON format.\n",
    "Documentation: json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHEUwF4Caina"
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Step 1: Data Extraction\n",
    "# File paths\n",
    "PDF_FILES = {\n",
    "    \"affiliated_colleges\": \"data1.pdf\",\n",
    "    \"admission_notification\": \"data2.pdf\"\n",
    "}\n",
    "CSV_FILE = \"uok_data.csv.txt\"\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF files.\"\"\"\n",
    "    from pdfminer.high_level import extract_text\n",
    "    try:\n",
    "        return extract_text(pdf_path)\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "def extract_text_from_csv(csv_path):\n",
    "    \"\"\"Extract text from CSV file.\"\"\"\n",
    "    with open(csv_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def generate_json():\n",
    "    \"\"\"Parse data from files and save as JSON.\"\"\"\n",
    "    data = {\n",
    "        \"affiliated_colleges\": extract_text_from_pdf(PDF_FILES[\"affiliated_colleges\"]),\n",
    "        \"admission_notification\": extract_text_from_pdf(PDF_FILES[\"admission_notification\"]),\n",
    "        \"general_info\": extract_text_from_csv(CSV_FILE)\n",
    "    }\n",
    "    # Save the data as JSON\n",
    "    with open(\"uok_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "    print(\"uok_data.json has been created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Scp-wbXrkDm4"
   },
   "source": [
    "# Step 2: Load and Preprocess Data\n",
    "\n",
    "This step involves loading the extracted data from the JSON file, splitting it into smaller chunks, and cleaning the data for further analysis.\n",
    "\n",
    "**Libraries and Technologies Used:**\n",
    "json (for loading the extracted data):\n",
    "\n",
    "**Purpose:** The json library is used again to load the JSON file that was created in Step 1, which contains all the extracted data from PDFs and CSVs.\n",
    "\n",
    "**Usage: **Loads the raw data into Python dictionaries or lists.\n",
    "\n",
    "**Documentation:** json\n",
    "pandas (for creating DataFrame and managing data):\n",
    "\n",
    "**Purpose:** pandas is used to handle the data in a tabular format (DataFrame) for easier manipulation and processing.\n",
    "\n",
    "**Usage:** Creates a structured DataFrame where each row contains a piece of text from the data and its corresponding label.\n",
    "\n",
    "**Documentation:** pandas\n",
    "spaCy (for text preprocessing and natural language processing):\n",
    "\n",
    "**Purpose:** spaCy is a powerful NLP library that is used for cleaning the text data. This includes tasks like tokenization, lemmatization, and removing stopwords.\n",
    "\n",
    "**Usage:** Used to preprocess the text by tokenizing and lemmatizing words, and removing unnecessary stopwords.\n",
    "\n",
    "**Documentation:** spaCy\n",
    "re (for text cleaning with regular expressions):\n",
    "\n",
    "**Purpose:** The built-in re library is used to perform text cleaning tasks, like removing special characters, punctuation, and unwanted patterns.\n",
    "\n",
    "**Usage:** Used for cleaning the text and removing any non-alphanumeric characters.\n",
    "Documentation: re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlsmDiDyamNf"
   },
   "outputs": [],
   "source": [
    "# ------------------ STEP 2: Load and Preprocess Data ------------------\n",
    "def load_data():\n",
    "    \"\"\"Load data from uok_data.json and prepare labeled dataset.\"\"\"\n",
    "    with open(\"uok_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Affiliated colleges\n",
    "    for line in raw_data[\"affiliated_colleges\"].split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            data.append(line.strip())\n",
    "            labels.append(\"college_info\")\n",
    "\n",
    "    # Admission notifications\n",
    "    for line in raw_data[\"admission_notification\"].split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            data.append(line.strip())\n",
    "            labels.append(\"admission\")\n",
    "\n",
    "    # General info\n",
    "    for line in raw_data[\"general_info\"].split(\".\"):\n",
    "        if line.strip():\n",
    "            data.append(line.strip())\n",
    "            labels.append(\"general_info\")\n",
    "\n",
    "    return pd.DataFrame({\"text\": data, \"label\": labels})\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text using spaCy.\"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    # Remove stopwords, punctuation, and non-alphanumeric tokens\n",
    "    tokens = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlYhHGM7kPmR"
   },
   "source": [
    "# Step 3: Train the ML Model\n",
    "\n",
    "In this step, we train a machine learning model (Logistic Regression) using the preprocessed text data to classify user queries.\n",
    "\n",
    "**Libraries and Technologies Used:**\n",
    "scikit-learn (for machine learning model training and evaluation):\n",
    "\n",
    "**Purpose:** scikit-learn is a comprehensive library for machine learning in Python. It is used here for model training, evaluation, and text vectorization.\n",
    "\n",
    "**Usage:**\n",
    "TF-IDF Vectorization: Converts text data into numerical form (vectors) that can be used by machine learning models.\n",
    "Logistic Regression: Trains a classifier to predict the category of a given query (e.g., college_info, admission, etc.).\n",
    "\n",
    "**Documentation:** scikit-learn\n",
    "TfidfVectorizer (for converting text data into numerical vectors):\n",
    "\n",
    "**Purpose:** This is a part of scikit-learn that converts text into numerical vectors using the Term Frequency-Inverse Document Frequency (TF-IDF) method.\n",
    "\n",
    "**Usage:** Transforms the cleaned text into a matrix of TF-IDF features for model training.\n",
    "\n",
    "**Documentation:** TfidfVectorizer\n",
    "LogisticRegression (for training a classification model):\n",
    "\n",
    "**Purpose:** LogisticRegression is used for text classification tasks. It is a simple yet effective classification algorithm for binary and multi-class classification problems.\n",
    "\n",
    "**Usage:** Trains a logistic regression model to classify text into predefined categories.\n",
    "\n",
    "**Documentation:** LogisticRegression\n",
    "train_test_split (for splitting the data into training and testing sets):\n",
    "\n",
    "**Purpose:** train_test_split splits the dataset into training and testing subsets to evaluate the performance of the model.\n",
    "Usage: Splits the data into 80% training data and 20% testing data.\n",
    "\n",
    "**Documentation:** train_test_split\n",
    "accuracy_score (for model evaluation):\n",
    "\n",
    "**Purpose:** This function calculates the accuracy of the model on the test data by comparing predicted values with actual values.\n",
    "\n",
    "**Usage:** Used to evaluate the performance of the trained model.\n",
    "Documentation: accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCf3r9DIjBpC"
   },
   "outputs": [],
   "source": [
    "# ------------------ STEP 3: Train the ML Model ------------------\n",
    "\n",
    "def train_model(df):\n",
    "    \"\"\"Train a logistic regression model using TF-IDF.\"\"\"\n",
    "    df[\"text_clean\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df[\"text_clean\"])\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Model Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "    return model, vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzWQkMRQkO0_"
   },
   "source": [
    "# Step 4: Chatbot Logic\n",
    "This step involves detecting the language of the user’s query, matching it with pre-defined responses, and using the trained model for any remaining queries.\n",
    "\n",
    "Libraries and Technologies Used:\n",
    "langdetect (for language detection):\n",
    "\n",
    "**Purpose**: The langdetect library is used to detect the language of the user's query.\n",
    "\n",
    "**Usage**: Ensures that the chatbot only responds to queries in English.\n",
    "\n",
    "**Documentation**: langdetect\n",
    "spaCy (for text processing):\n",
    "\n",
    "**Purpose**: spaCy is again used here for tokenization, lemmatization, and stopword removal in the chatbot logic.\n",
    "\n",
    "**Usage**: Cleans the user’s input before feeding it to the model.\n",
    "\n",
    "**Documentation**: spaCy\n",
    "Custom Logic (for handling specific queries):\n",
    "\n",
    "**Purpose**: Custom Python code to handle frequently asked questions (e.g., \"Tell me about the university\", \"How many courses are available?\").\n",
    "\n",
    "**Usage**: Direct responses are returned if certain keywords are detected in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt331NsQiULk"
   },
   "outputs": [],
   "source": [
    "# ------------------ STEP 4: Chatbot Logic ------------------\n",
    "\n",
    "def detect_language(query):\n",
    "    \"\"\"Detect the language of the user's query.\"\"\"\n",
    "    try:\n",
    "        return detect(query)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def get_custom_response(query):\n",
    "    \"\"\"Provide specific responses for common questions.\"\"\"\n",
    "    query = query.lower()\n",
    "    if \"tell me about\" in query:\n",
    "        return \"The department is devoted to impart knowledge as well as social values for overall development.\"\n",
    "    elif \"how many courses\" in query:\n",
    "        return \"There are multiple courses, including programs in Arts, Science, Commerce, and more.\"\n",
    "    elif \"how many subjects\" in query:\n",
    "        return \"Subjects include Chemistry, Physics, Mathematics, Botany, and Zoology, among others.\"\n",
    "    elif \"how many departments\" in query:\n",
    "        return \"The university has departments like Science, Arts, Commerce, Law, and more.\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_response(query, model, vectorizer, df):\n",
    "    \"\"\"Get a response based on the user's query.\"\"\"\n",
    "    lang = detect_language(query)\n",
    "    if lang != \"en\":\n",
    "        return \"Sorry, I can only respond to queries in English.\"\n",
    "\n",
    "    # Check for specific responses\n",
    "    custom_response = get_custom_response(query)\n",
    "    if custom_response:\n",
    "        return custom_response\n",
    "\n",
    "    # Use the ML model for general queries\n",
    "    query_clean = preprocess_text(query)\n",
    "    query_vectorized = vectorizer.transform([query_clean])\n",
    "    predicted_label = model.predict(query_vectorized)[0]\n",
    "\n",
    "    # Fetch a relevant response\n",
    "    response_data = df[df[\"label\"] == predicted_label]\n",
    "    return response_data.sample(1)[\"text\"].values[0]\n",
    "\n",
    "\n",
    "def chat_with_user(model, vectorizer, df):\n",
    "    \"\"\"Interactive chat session.\"\"\"\n",
    "    print(\"Welcome to the University of Kota Chatbot!\")\n",
    "    print(\"Ask me anything about affiliated colleges, admissions, or general information.\")\n",
    "    print(\"Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() == \"exit\":\n",
    "            print(\"Chatbot: Goodbye! Have a great day!\")\n",
    "            break\n",
    "\n",
    "        # Get chatbot response\n",
    "        response = get_response(user_query, model, vectorizer, df)\n",
    "        print(f\"Chatbot: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir8MVHltkctV"
   },
   "source": [
    "# Step 5: Main Function\n",
    "\n",
    "This is the execution point where the above functions are brought together to create a chatbot application.\n",
    "\n",
    "**Libraries and Technologies Used:**\n",
    "Built-in Python Modules: Used for file handling (json, os), exception handling, and program flow control.\n",
    "\n",
    "input(): For interactive user input during the chat session.\n",
    "\n",
    "print(): For displaying chatbot responses in the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYxJrIWEiW6Z",
    "outputId": "8a7aac86-9e94-4682-aa20-08314d6e72dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data...\n",
      "uok_data.json has been created successfully!\n",
      "Loading data...\n",
      "Training model...\n",
      "Model Accuracy: 84.73%\n",
      "Welcome to the University of Kota Chatbot!\n",
      "Ask me anything about affiliated colleges, admissions, or general information.\n",
      "Type 'exit' to end the chat.\n",
      "\n",
      "Chatbot: Objectives of NSS for Student Volunteers\n",
      "\n",
      "understand the community in which they work\n",
      "understand themselves in relation to their community;\n",
      "identify the needs and problems of the community and involve them in problem solving process;\n",
      "develop among themselves a sense of social and civic responsibility;\n",
      "utilize their knowledge in finding practical solution to individual and community problems    \n",
      "develop competence required for group living and sharing of responsibilities;\n",
      "gain skills in mobilizing community participation;\n",
      "acquire leadership qualities and democratic attitude;\n",
      "develop capacity to meet emergencies and natural disasters\n",
      "practice national integration and social harmony\n",
      "\n",
      "Chatbot: There are multiple courses, including programs in Arts, Science, Commerce, and more.\n",
      "\n",
      "Chatbot: Hence, globally occupying the most prominent place among the thrust area activities, even recognized by the department of Science and Technology (DST), Government of India, that needs to be emphasized on in an integrated fashion by actively involving the industrial sector in general and energy generation plants in particular in order to contribute towards solving the problem of severe energy crisis and the environment related issues\n",
      "\n",
      "Chatbot: Bhawani Shankar Meena , the member of University Boxing team  (Men)  selected to represent University in All India Inter University Boxing Championship and qualify for Gold Medal in 2004\n",
      "\n",
      "Chatbot: 7742288786\n",
      "\n",
      "Chatbot: B.Ed.\n",
      "\n",
      "Chatbot: Sorry, I can only respond to queries in English.\n",
      "\n",
      "Chatbot: Sorry, I can only respond to queries in English.\n",
      "\n",
      "Chatbot: To promote the attitude to serve the society\n",
      "To promote learning and research aptitude\n",
      "Ph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 5: Main Function ------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Generate JSON file from raw data\n",
    "        print(\"Extracting data...\")\n",
    "        generate_json()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        print(\"Loading data...\")\n",
    "        df = load_data()\n",
    "\n",
    "        # Train ML model\n",
    "        print(\"Training model...\")\n",
    "        model, vectorizer = train_model(df)\n",
    "\n",
    "        # Start chatbot\n",
    "        chat_with_user(model, vectorizer, df)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "        print(\"Please ensure the required files are in the same directory.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
